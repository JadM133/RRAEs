\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
% \usepackage[nonatbib, preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[nonatbib, final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib]{neurips_2024}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage[numbers]{natbib}
\usepackage[ruled]{algorithm2e}
\usepackage{amsmath}
\usepackage{multicol}
\usepackage{tikz}
\usepackage[dvipsnames]{xcolor}
\usepackage{hhline}
\usepackage{booktabs}
\usepackage{float}
\usepackage[title]{appendix}
\graphicspath{{../figures/}}
\definecolor{myblue}{RGB}{74, 144, 226}
\definecolor{myred}{RGB}{255, 2, 27}

\title{Rank Reduction Autoencoders - Enhancing interpolation on nonlinear manifolds.}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Jad Mounayer\\
  ENSAM Institute of Technology\\
  PIMM, SKF Chair\\
  Blvd de l'H么pital, Paris \\
  \texttt{jad.mounayer@outlook.com}\\
  % examples of more authors
  \And
  Sebastian Rodriguez\\
  ENSAM Institute of Technology\\
  PIMM, RTE Chair\\
  Blvd de l'H么pital, Paris\\
  \texttt{sebastian.rodriguez\_iturra@ensam.eu}\\
  \AND
  Chady Ghnatios \\
  ENSAM Institute of Technology\\
  PIMM, SKF Chair\\
  Blvd de l'H么pital, Paris\\
  \texttt{chady.ghnatios@ensam.eu}\\
  \And
  Charbel Farhat \\
  Department of Aeronautics and Astronautics,\\ Stanford University,\\
  Stanford, 94305,\\
  \texttt{cfarhat@stanford.edu}\\
  \And
  Francisco Chinesta \\
  ENSAM Institute of Technology\\
  PIMM, ESI/Keysight Chair\\
  CNRS@CREATE, Singapore\\
  Blvd de l'H么pital, Paris\\
  \texttt{francisco.chinesta@ensam.eu} \\
}


\begin{document}


\maketitle


\begin{abstract}
    The efficiency of classical Autoencoders (AEs) is limited in many practical situations. When the latent space is reduced through autoencoders, feature extraction becomes possible. However, overfitting is a common issue, leading to 'holes' in AEs' interpolation capabilities. On the other hand, increasing the latent dimension results in a better approximation with fewer non-linearly coupled features (e.g., Koopman theory or kPCA), but it doesn't necessarily lead to dimensionality reduction, which makes feature extraction problematic. As a result, interpolating using Autoencoders gets harder. In this work, we introduce the Rank Reduction Autoencoder (RRAE), an autoencoder with an enlarged latent space, which is constrained to have few dominant singular values (i.e., low-rank). The latent space of RRAEs is large enough to enable accurate predictions while enabling efficient feature extraction. As a result, the proposed autoencoder features a minimal rank linear latent space. To achieve what's proposed, two formulations are presented, a strong and a weak one, that build a reduced basis that accurately represents the latent space. The first formulation consists of a truncated SVD in the latent space, while the second one adds a penalty term to the loss function. We show the efficiency of our formulations by using both of them for interpolation tasks and comparing the results to state-of-the-art autoencoders.
\end{abstract}

\section{Introduction}\label{sec:intro}
Interpolation of vector functions over a parametric space is an active research topic since accurate interpolation allows the reconstruction of a physical solution in an entire parametric space from a set of pre-computed samples. Multiple techniques have been proposed to perform interpolation, mainly the Proper Orthogonal Decomposition with Interpolation (PODI) \citep{tezzele2019shape, nguyen2022efficient, rama2020towards}, or the sparse-PGD (sPGD) \cite{chinesta2011short, sancarlos2023regularized}. Most of these techniques are based on Model Order Reductions, such as the Proper Orthogonal Decomposition (POD) \cite{rowley2004model, kerschen2005method}, the Proper Generalized Decomposition (PGD) \cite{ladeveze1985famille,rodriguez2019non, chinesta2014pgd}, and the Principal Component Analysis (PCA) \cite{labrin2020principal, gonzalez2018kpca}. These techniques stack the vector functions in what is called the solution matrix, and their efficiency is inversely proportional to the rank of this matrix. If the solution matrix only has a few dominant singular values (i.e., low-rank), it is easier for the aforementioned techniques to reduce the problem and interpolate. However, when this assumption does not apply, they fail to define an efficient surrogate for the correct prediction of physical phenomena. A high-rank solution matrix is also problematic for other techniques based on different formulations such as those based on Grassmann manifolds \cite{grasmanian_interp}, the Optimal Transport (OT) \cite{TORREGROSA202212, TORREGROSA202236}, or every low-rank technique (e.g. \cite{srebro2003weighted, liu2012robust, udell2016generalized, davenport2016overview}).

On the other hand, autoencoders with Neural Networks as their encoding and decoding functions are tempting to use for interpolation because of their ability to capture nonlinear behavior. They have been used in different applications such as speech recognition \cite{vachhani2017deep, feng2014speech}, medical applications \cite{deng2019towards}, robotics \cite{park2018multimodal}, and others \cite{bank2023autoencoders}. Yet, the efficiency of Vanilla Autoencoders is limited. On the one hand, Autoencoders with reduced latent spaces (or Diabolo Autoencoders (DAEs)), can easily overfit the data and hence are known for having ``holes'' in their latent spaces. On the other hand, even though enlarged latent spaces lead to better approximations with less non-linear behavior (by the Koopman theory or the kPCA), the representations learned are usually of a large dimension which limits both interpolation and feature extraction. 

% This has led to multiple enhancements such as Variational \cite{doersch2016tutorial}, or Sparse Autoencoders \cite{ng2011sparse} which improved Autoencoders overall but did not definitively solve the interpolation issues.

Neural Networks are increasingly being used for nonlinear reduction techniques \cite{barnett2022neuralnetworkaugmented}. Recently, the Implicit Rank-Minimizing Autoencoder (IRMAE) \cite{jing2020implicit}, and the Low-Rank Autoencoder (LoRAE) \cite{mazumder2023learning} showcased how increasing the latent space dimension while encouraging a low-rank achieves better results, including interpolation. If the latent space has a low rank, the efficiency of all presented interpolation techniques (including basic linear interpolation) in the latent space is enhanced. The resulting Autoencoder would benefit from the large data dimensionality of the latent space to find better approximations while allowing feature extraction because of its low rank. The architecture of IRMAE consists of adding linear layers between the encoder and the latent space, while LoRAE only adds one linear layer as well as its nuclear norm as a penalty for the loss. While both papers show how their resulting latent spaces have a lower rank compared to Vanilla and Variational Autoencoders, their work has some limitations. First, while both architectures can find a low-rank latent space with singular values that are sharply decreasing, they do not force the small singular values to go to zero. Accordingly, the decoder always has some noise from the small singular values, even though ideally, we would like to remove their effect entirely. In addition, the computational time of both architectures highly depends on the latent space dimension $L$. Since a long latent space is crucial for achieving better results, both IRMAEs and LoRAEs can be computationally expensive. Finally, both architectures do not provide explicit control over the rank of the latent space. While they have tuning parameters, we show later in the paper that their proposed parameters can not reach a rank that is as low as we desire.

In this paper, we present Rank Reduction Autoencoders (RRAEs), which have large latent spaces restricted to have a low rank. By enforcing the latent space to accept a linear reduction (hence a lower rank), we show that our model resolves the issues previously mentioned. Our architecture includes two proposed formulations: (i) a strong and (ii) a weak one. Throughout the paper, we show that the strong formulation finds orthogonal basis vectors and forms a reduced POD basis, while the weak formulation is allowed to find non-orthogonal ones. We show that both formulations can interpolate efficiently between high-rank synthetic solutions while achieving a lower rank than the IRMAE and the LoRAE. We also demonstrate how both formulations can overcome other architectures when interpolating over the MNIST dataset, with a significantly lower rank.

The present paper is structured as follows: section \ref{sec:HT} presents the architecture and both proposed formulations. Section \ref{sec:insights} explains the insights behind long latent spaces with a low rank on two synthetic examples. Then, we compare the interpolation capabilities of RRAEs with IRMAEs, and LoRAEs on a variety of problems in section \ref{sec:Nuemrical}. We explain the limitations of the proposed formulations in section \ref{sec:limitations}, before finally summarizing the main original contributions of the present paper in section \ref{sec:conclusions}.

\section{Rank Reduction Autoencoders (RRAEs)}\label{sec:HT}
To define the architecture of RRAEs, we begin by defining autoencoder notations. Let $\{X_i\}_{i\in [1, D]} \in \mathbb{R}^T$ be a set of $D$ series of observations, each having $T$ degrees of freedom. We define our input $X\in \mathbb{R}^{T\times D}$ with $X_i$ as its $i$th column. Let, $Y \in \mathbb{R}^{L\times D}$, with $L$, the chosen dimension\footnote[1]{See Appendix \ref{apdx:train} for details on the choice of $L$.} of the latent space. We also define the encoding map $e: \mathbb{R}^{T\times D} \xrightarrow{} \mathbb{R}^{L\times D}$ and the decoding map $d: \mathbb{R}^{L\times D} \xrightarrow{} \mathbb{R}^{T\times D}$. The Vanilla autoencoder can be written as the following two operations,
\begin{equation}
    Y = e(X), \qquad \tilde{X} = d(Y).
\end{equation}
In practice, we usually enforce that the output of the autoencoder gives us back the original data, hence the loss $\mathcal{L}$ usually reads,
\begin{equation}
    \mathcal{L}(X, \tilde{X}) = \Vert X-\Tilde{X}\Vert_2, \quad \text{where,} \quad \Vert \cdot\Vert_2 \text{ is the $L2$-norm}.
\end{equation}
The idea behind RRAEs is to enforce the latent matrix to have a low rank while finding a reduced basis. In other words, if $Y$ has a rank $r$, let $Y = U\Sigma V$ be the Singular Value Decomposition (SVD) \cite{stewart1993early, nakatsukasa2017accuracy} of $Y$, with $U\in\mathbb{R}^{L\times r}$, $\Sigma \in\mathbb{R}^{r\times r}$, and $V\in\mathbb{R}^{r\times D}$. Let $\{\sigma_i\}_{i \in [1, r]}$ be the sorted diagonal values of $\Sigma$. Thus, by considering the $k$ most significant modes (choice of $k$ detailed in Appendix \ref{apdx:train}), it results,
\begin{equation}\label{eqn:trunc}
    Y = \sum_{i=1}^r\sigma_iU_iV_i^T \qquad\Rightarrow\qquad Y\approx \sum_{i=1}^k\sigma_iU_iV_i^T, \qquad k\ll r,
\end{equation}
where $U_i$ is the $i$th column of $U$ and $V_i^T$ is the $i$th row of $V$. In other words, we can write $Y_d$, the $d$th column of $Y$ as,
\begin{equation}\label{eqn:alphas}
    Y_d \approx \sum_{i=1}^k\left(\sigma_iU_iV_i^T\right)_d = \sum_{i=1}^k\sigma_iU_iV_{i,d} = \sum_{i=1}^k\alpha_{i,d}U_i, \qquad\quad \forall d \in [1, D],
\end{equation}
with $V_{i,d}$ being entry $d$ of vector $V_i^T$.

Accordingly, for $k$ modes, each column of $Y$ is defined by $k$ coefficients and $k$ vectors. Further, vectors $U_i$ form a basis for the latent space. We can write \eqref{eqn:alphas} in matrix form as follows,
\begin{equation}\label{eqn:mat_form}
    Y \approx UA, \qquad \text{with: } A_{i,j}=\alpha_{i,j}, \quad U \in \mathbb{R}^{L\times k}. \quad A \in \mathbb{R}^{k\times D},
\end{equation}
Based on \eqref{eqn:alphas}, and \eqref{eqn:mat_form}, we propose two formulations that enforce the low rank of the latent space and find its reduced basis. The architecture is sketched in Figure \ref{fig:mdoel_arch}.
\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt 


\begin{figure}



    \tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

    \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1, scale=0.9]
        %uncomment if require: \path (0,329); %set diagram left start at 0, and has height of 329

        %Straight Lines [id:da21499615368111846] 
        \draw    (37,108.56) -- (37,203.67) ;
        %Straight Lines [id:da16326946357296768] 
        \draw    (37,108.56) -- (47.89,108.56) ;
        %Straight Lines [id:da16265931324232152] 
        \draw    (37,203.67) -- (47.89,203.67) ;
        %Straight Lines [id:da5083389594267504] 
        \draw    (116.56,204.33) -- (116.56,108.67) ;
        %Straight Lines [id:da3615318503641125] 
        \draw    (117.22,203.7) -- (106.33,203.7) ;
        %Straight Lines [id:da9354769223516279] 
        \draw    (116.22,108.6) -- (105.33,108.6) ;
        %Straight Lines [id:da9284204806788381] 
        \draw    (57.67,108.87) -- (57.07,203.27) ;
        %Straight Lines [id:da9687380676381745] 
        \draw    (76.67,109.25) -- (76.67,203.25) ;
        %Straight Lines [id:da30039595434321] 
        \draw    (96.67,109.25) -- (96.67,203.25) ;
        %Straight Lines [id:da782396384119824] 
        \draw    (124.67,139.67) -- (180.67,121.42) ;
        %Straight Lines [id:da9238744798761502] 
        \draw    (124.67,169) -- (180.67,186) ;
        %Straight Lines [id:da44599235940336523] 
        \draw    (180.67,121.42) -- (180.67,186.33) ;
        %Straight Lines [id:da6964885949042201] 
        \draw    (213.3,95.06) -- (213.3,224.17) ;
        %Straight Lines [id:da06959562543054565] 
        \draw    (213.3,95.06) -- (224.19,95.06) ;
        %Straight Lines [id:da030633764479459424] 
        \draw    (213.3,224.17) -- (224.19,224.17) ;
        %Straight Lines [id:da3263511981591738] 
        \draw    (293.52,223.5) -- (293.52,96.17) ;
        %Straight Lines [id:da40502884422960217] 
        \draw    (293.52,223.5) -- (282.63,223.5) ;
        %Straight Lines [id:da8961788198500589] 
        \draw    (293.52,96.3) -- (282.63,96.3) ;
        %Straight Lines [id:da9774520960600939] 
        \draw    (232.97,95.37) -- (232.37,223.77) ;
        %Straight Lines [id:da5075505519042733] 
        \draw    (252.97,95.75) -- (252.97,223.75) ;
        %Straight Lines [id:da21045855349115494] 
        \draw    (272.97,95.75) -- (272.97,222.75) ;
        %Straight Lines [id:da754710144489299] 
        \draw    (124.67,139.67) -- (124.67,169) ;
        %Straight Lines [id:da7099185888654047] 
        \draw    (417.17,119.92) -- (476.67,139.42) ;
        %Straight Lines [id:da66143724081499] 
        \draw    (476.67,168.75) -- (416.67,188.92) ;
        %Straight Lines [id:da31064466545771574] 
        \draw    (417.17,119.92) -- (416.67,188.92) ;
        %Straight Lines [id:da5393972903258972] 
        \draw    (476.67,139.42) -- (476.67,168.75) ;
        %Straight Lines [id:da022156106633165695] 
        \draw    (296.67,156.67) -- (311.67,156.67) ;
        \draw [shift={(314.67,156.67)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
        %Straight Lines [id:da38115442380124565] 
        \draw    (395.67,156.67) -- (410.67,156.67) ;
        \draw [shift={(415.67,156.67)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
        %Shape: Rectangle [id:dp7709069667907207] 
        \draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (317.4,131.47) -- (393.67,131.47) -- (393.67,182.67) -- (317.4,182.67) -- cycle ;
        %Shape: Rectangle [id:dp24666081940562967] 
        \draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] (300.6,248) -- (405.89,248) -- (405.89,280.67) -- (300.6,280.67) -- cycle ;
        %Straight Lines [id:da9073615867889311] 
        \draw    (27.8,111.67) -- (27.8,200) ;
        \draw [shift={(27.8,203)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
        \draw [shift={(27.8,108.67)}, rotate = 90] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (10.72,-5.15) -- (0,0) -- (10.72,5.15) -- (7.12,0) -- cycle    ;
        %Straight Lines [id:da1988310735559562] 
        \draw    (203.6,98.87) -- (203.6,220.87) ;
        \draw [shift={(203.6,223.87)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
        \draw [shift={(203.6,95.87)}, rotate = 90] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (10.72,-5.15) -- (0,0) -- (10.72,5.15) -- (7.12,0) -- cycle    ;
        %Straight Lines [id:da7075949415859115] 
        \draw    (486.33,108.56) -- (486.33,203.67) ;
        %Straight Lines [id:da776447423867771] 
        \draw    (486.33,108.56) -- (497.22,108.56) ;
        %Straight Lines [id:da40114415684531135] 
        \draw    (486.33,203.67) -- (497.22,203.67) ;
        %Straight Lines [id:da24114735322590564] 
        \draw    (565.89,204.33) -- (565.89,108.67) ;
        %Straight Lines [id:da8606039915940866] 
        \draw    (566.56,203.7) -- (555.67,203.7) ;
        %Straight Lines [id:da6175875067439736] 
        \draw    (565.56,108.6) -- (554.67,108.6) ;
        %Straight Lines [id:da9114789567469446] 
        \draw    (507,108.87) -- (506.4,203.27) ;
        %Straight Lines [id:da9606142652454763] 
        \draw    (526,109.25) -- (526,203.25) ;
        %Straight Lines [id:da3463806230867499] 
        \draw    (546,109.25) -- (546,203.25) ;
        %Straight Lines [id:da3635085614733373] 
        \draw    (576.02,111.67) -- (576.02,200) ;
        \draw [shift={(576.02,203)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
        \draw [shift={(576.02,108.67)}, rotate = 90] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (10.72,-5.15) -- (0,0) -- (10.72,5.15) -- (7.12,0) -- cycle    ;

        % Text Node
        \draw (38,151.2) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$X_{1}$};
        % Text Node
        \draw (58,151.2) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$X_{2}$};
        % Text Node
        \draw (97,151.2) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$X_{D}$};
        % Text Node
        \draw (127.33,149.67) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {Encoder};
        % Text Node
        \draw (216.3,151.23) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$Y_{1}$};
        % Text Node
        \draw (234.3,151.23) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$Y_{2}$};
        % Text Node
        \draw (275.3,150.73) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$Y_{D}$};
        % Text Node
        \draw (421.33,148.17) node [anchor=north west][inner sep=0.75pt]  [font=\footnotesize] [align=left] {Decoder};
        % Text Node
        \draw (301,101.2) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize,color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ] [align=left] {\begin{minipage}[lt]{69.37pt}\setlength\topsep{0pt}
                \begin{center}
                    Only for the \\strong formulation
                \end{center}

            \end{minipage}};
        % Text Node
        \draw (284.8,233.2) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize,color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ] [align=left] {Only for the weak formulation};
        % Text Node
        \draw (12,147.6) node [anchor=north west][inner sep=0.75pt]    {$T$};
        % Text Node
        \draw (188.8,149.8) node [anchor=north west][inner sep=0.75pt]    {$L$};
        % Text Node
        \draw (317.2,134) node [anchor=north west][inner sep=0.75pt]    {$\sum \limits_{i=1}^{k_{max}} \hspace{-0.13cm}\sigma _{i} U_{i} V_{i}^{T}$};
        % Text Node
        \draw (173.07,251.73) node [anchor=north west][inner sep=0.75pt]    {$\mathcal{L} =\ \lambda _{1} \| \ X-\tilde{X} \ \Vert _{2} +\ \lambda _{2} \| Y-UA\Vert $};
        % Text Node
        \draw (76,154.07) node [anchor=north west][inner sep=0.75pt]    {$\dotsc $};
        % Text Node
        \draw (252.13,154.07) node [anchor=north west][inner sep=0.75pt]    {$\dotsc $};
        % Text Node
        \draw (487.33,147.2) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$\tilde{X}_{1}$};
        % Text Node
        \draw (525.13,154.07) node [anchor=north west][inner sep=0.75pt]    {$\dotsc $};
        % Text Node
        \draw (508,147.2) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$\tilde{X}_{2}$};
        % Text Node
        \draw (546.33,147.2) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$\tilde{X}_{D}$};
        % Text Node
        \draw (578.56,147.6) node [anchor=north west][inner sep=0.75pt]    {$T$};


    \end{tikzpicture}

    \caption{Schematic showing the autoencoder in use as well as both methodologies. There are two terms in the loss function for the \textcolor{myblue}{Weak formulation}. On the other hand, there's an additional step before the decoder for the \textcolor{myred}{Strong formulation}.}
    \label{fig:mdoel_arch}
\end{figure}

\begin{enumerate}
    \item \underline{The Weak formulation:} After choosing the maximum allowed number of modes $k_{max}$, we generate two trainable matrices $A \in \mathbb{R}^{k_{max}\times D}$, and $U \in \mathbb{R}^{L\times k_{max}}$. Afterward, we add a term to the loss as seen in blue in Figure \ref{fig:mdoel_arch}. By doing so, minimizing the loss means that the latent space would have at most a rank of $k_{max}$. After convergence, the columns of our trainable matrix $U$ form the reduced basis of the latent space. Additionally, the coefficients found in matrix $A$ describe how to reconstruct each column $Y_d$ as a linear combination of the basis vectors. We will refer to this method as the Weak formulation since throughout training, the Network minimizes a sum of both terms and not each term individually. Accordingly, predictions $\tilde{X}$ could be less accurate, and  we might end up with more modes than the specified value of $k_{max}$.

          \underline{Remark:} The two trainable matrices can be computed from a one-rank greedy procedure, as PGD performs.
    \item \underline{The Strong formulation:} Unlike the weak formulation, this architecture enforces, in a strong manner, the maximum dimension of the reduced basis of the latent space. Similarly to the weak formulation, we begin by choosing the maximum rank $k_{max}$ of the latent space. Then, as seen in red in Figure \ref{fig:mdoel_arch}, a truncated SVD (of order $k_{max}$) of the latent space is given to the decoder, instead of the latent space itself. Accordingly, the input of the decoder will have at most $k_{max}$ dominant singular values. We refer to this method as the Strong formulation since we strictly enforce the latent space to have a rank that's lower or equal to $k_{max}$. In this case, the basis vectors and coefficients are simply the ones found by the truncated SVD.
\end{enumerate}

In both formulations, $k_{max}$ is a hyperparameter to be chosen. We propose a strategy to choose this hyperparameter and discuss its effect in Appendix \ref{apdx:train}.

When using the strong formulation, we compute a POD basis since the vectors are by construction orthogonal. The orthogonality of the basis vectors, as well as refraining from adding terms in the loss, can make both the training and interpolation better. On the other hand, backpropagation through the singular value decomposition is not common in practice. All the work presented in this paper was performed using \texttt{equinox} in \texttt{JAX}, where gradients of the singular value decomposition are implemented and accessible.

Both formulations reduce the limitations of IRMAE and LoRAE. We sum up our contributions as follows:
\begin{enumerate}
    \item RRAEs with a strong formulation lead to low-rank latent spaces that have many singular values exactly equal to zero. In other words, the decoder will get a sum of exactly $k_{max}$ rank-one updates. As will be shown later in the paper, this gives the strong formulation an advantage for training and interpolation.
    \item The computational overhead of RRAEs can be much lower, especially for large latent spaces. For the strong formulation, when batches are used, the SVD is only performed on a matrix of size $L\times bs$, $bs$ being the batch size. Similarly, for the weak formulation, the added computational cost is minimal since the trainable matrices are of shape $L \times k_{max}$ and $k_{max} \times D$ with $k_{max} \ll L$. On the other hand, the IRMAE or the LoRAE either perform a gradient descent or find the nuclear norm of an $L\times L$ matrix. Since a large latent space dimension $L$ usually helps in achieving better results, both IRMAEs and LoRAEs can be computationally challenging. 
    \item Both formulations give us explicit control over the rank of the latent space. As shown next in the paper, we can control the latent space to have a lower rank than IRMAE and LoRAE, which leads to better interpolation and could help for feature extraction in future applications.
\end{enumerate}

% \section{Interpolation in the latent space}\label{sec:interp}
% The fact that motivated us to make this proposal is the limitations of linear interpolation between solutions when the solution matrix is high-rank. Take, for instance, sinusoidal curves shifted by a scalar $p$ (i.e. $\sin(x+p)$). As can be seen in Figure \ref{fig:sin}, interpolating linearly between the curves corresponding to parameters $p_0 = 0$ and $p_1 = \pi$ to find the middle curve at $p^*= \pi/2$ (i.e. simply the sum divided by two) leads to the horizontal line at zero, instead of finding the correct curve shifted towards the middle.


% \begin{figure}[!b]
%     \centering
%     \includegraphics[clip, trim=0.2cm 0.3cm 0cm 1cm, width=0.54\textwidth]{sin.pdf}
%     \caption{Result when interpolating linearly between two shifted sine curves, showing why linear interpolation is not a good choice for problems with multiple dominant singular values.}
%     \label{fig:sin}
% \end{figure}


% By mapping the data into a larger latent space while enforcing a low rank, we can find a space where a linear interpolation would be possible to represent the interpolated curve, before going back to the original space, using the decoder.

% Both proposed formulations allow us to approximate the latent space by \eqref{eqn:alphas}. Now let's suppose that every series of observations $X_d$ is tied to a vector of parameters $\mathbf{p}_d \in \mathbb{R}^P$. Accordingly, and since the basis vectors (i.e. columns of $U$) are the same for any solution, we can interpolate between the coefficients $\alpha_{j,d}$,
% \begin{equation}
%     Y_d=\sum_{j=1}^{k_{max}}\alpha_{j,d}U_j \qquad \Longrightarrow \qquad Y_d(\textbf{p}_d) = \sum_{j=1}^{k_{max}}\gamma_j(\textbf{p}_d)U_j,
% \end{equation}
% where each $\gamma_j: \mathbb{R}^P \xrightarrow{} \mathbb{R}$, could be any mapping, that maps all the training parameters to the corresponding $\alpha_{j,d}$, and allows us to interpolate when used on new values of $\mathbf{p}$. Since we interpolate between coefficients, the computational cost of RRAEs for interpolation is the same as the cost of using a Vanilla Autoencoder with a latent space of dimension $k_{max}$.

% For instance, for a parameter space of dimension one and sorted coefficients, we can interpolate linearly by writing,
% \begin{equation}
%     \gamma_j(p) = \alpha_{j,m} + \frac{\alpha_{j, m+1}-\alpha_{j,m}}{p_{m+1}-p_{m}}(p-p_{m}), \qquad \forall p \in [p_m, p_{m+1}].
% \end{equation}
% In this paper, we show that interpolating linearly for parametric spaces of dimension one, and bilinearly for those of dimension two is enough. In general, each $\gamma_j$ can be approximated by any regression, including a Neural Network, or the sPGD.
\section{Insights behind Long latent spaces with low rank}\label{sec:insights}
An enlarged latent space can exhibit a linear behavior (as employed for instance in the Koopman theory, or the kPCA). Furthermore, a latent space with a reduced basis allows us to interpolate better, as well as extract features. The Diabolo Autoencoder on the other hand has ``holes'' in its interpolation \cite{jing2020implicit}, since it does not find a basis, but only a set of coefficients that are helpful for the decoder to retrieve the solution. Since the decoder is highly nonlinear, the coefficients can be anything, which leads to overfitting.

To illustrate the aforementioned arguments, we test DEAs and our Strong formulation on two examples characterized by one parameter. The first curves we propose are shifted sine curves since these have a simple nonlinearity, but they are hard to separate (nonmonotonic and cross each other multiple times). For our second example, we chose curves with stair-like behavior. In that case, we create highly nonlinear curves (different supports, different numbers of jumps of different magnitudes), but we define them to be monotonic and only cross each other occasionally (i.e. easier to separate). The equations used to define the columns of our input matrix $X$ in each case are as follows,
\begin{equation}\nonumber
    \begin{cases}
        X_d(t_v,\, p_d) = f_{shift}(t_v, \, p_d) = \sin(t_v-p_d\pi), \hspace{0.1cm}\qquad\quad & p_d \in [0, 1.7], \\[1.2ex]
        X_d(t_v, \,p_d) = f_{stair}(t_v,\, p_d, \, \text{args}) \qquad\quad                    & p_d \in [1, 5],
    \end{cases}
\end{equation}
where $t_v \in \mathbb{R}^T$ is a vector of time at which observations are done, and $f_{stair}$ takes some arguments ``args'' as detailed in the algorithm in Appendix \ref{apdx:stair}. Training is performed over 17, and 40 equidistant values of $p_d$ for the shifted sine curves and the stair-like curves respectively. We then interpolate in the latent space of the Autoencoders on 80 and 300 random values of $p_d$, respectively, chosen inside the training domain. The large number of tests guarantees that the models are learning the dynamics and not just the training curves and some tests nearby. Since the solution curves depend on one parameter, we use a DAE with a single scalar latent space and an RRAE with a longer latent space of rank one. We then linearly interpolate in the latent space to predict the test set. The training parameters, including the dimension of the latent space, can be found in Appendix \ref{apdx:train}. The relative error over all $p_d$ values for both the train and test sets is summarized in Table \ref{fig:table_shift_sin}. Further, for each example, predictions over some tests are plotted in Figure \ref{fig:sin_shift_test}.



\begin{table}[!h]
    \caption{Relative error (in \%) for all three architectures on both the train and test sets for both the examples of shifted sin curves and stair-like ones.}
    \label{fig:table_shift_sin}
    \centering
    \begin{tabular}{ccccc}
        \toprule
        {}            & \multicolumn{2}{c}{Shifted sine} & \multicolumn{2}{c}{Stair-like}                                \\
        \cmidrule(lr){2-5}                                                                                               \\
        \addlinespace[-0.2cm]
        Model         & Train Error                      & Test error                     & Train Error   & Test error   \\
        \midrule
        DAE    &    2.12                         & 32.42                          & 2.97          & 3.74         \\
        RRAE (strong) & \textbf{1.73}                    & \textbf{1.90}                   & \textbf{1.87} & \textbf{3.2} \\
        \bottomrule
    \end{tabular}
\end{table}


The results show that when curves are hard to separate, RRAEs are better interpolators than DAEs. On the other hand, the effect of longer latent spaces is reduced for simple curves that can be highly nonlinear, but characterized by one parameter, and easily separable.


\begin{figure}[!t]
    \centering
    \includegraphics[clip, scale=0.3, trim=3cm 1cm 0cm 2cm]{sin_shift_plot_test.pdf}
    \caption{Predictions of Vanilla Autoencoders and RRAEs with both formulations over two particular values of $p_d$ for the shifted sine (above) and the stair-like examples (below).}
    \label{fig:sin_shift_test}
\end{figure}


\begin{figure}[!b]
    \centering
    \includegraphics[clip, scale=0.25, trim=-1cm 1cm 0cm 3.2cm]{sin_shift_plot_latent.pdf}
    \caption{Normalized Coefficients to be interpolated (dots) for DAE and RRAE with a strong formulation, and the interpolated values for the test set (crosses) for the shifted sine curves (left) and the stair-like curves (right).}
    \label{fig:sin_shift_latent}
\end{figure}


To further investigate the results, we plot the coefficients to be interpolated in the latent space as a function of the corresponding parameter in Figure \ref{fig:sin_shift_latent}. It is important to note that the coefficients are defined differently between the RRAE and the DAE. For RRAEs, when $k_{max} = 1$,  the coefficients are simply the entries of $A \in \mathbb{R}^D$ in \eqref{eqn:mat_form}. On the other hand, for a Diabolo Autoencoder with a scalar latent space, the values in the latent space themselves are the coefficients.

The main problem with the coefficients found by the DAE for the shifted sine curves (the blue crosses and dots in Figure \ref{fig:sin_shift_latent} (left)) is that the resulting curve from linearly interpolating the coefficients is not an injection, over two significant parts of the domain. Specifically, for any value of $p_d$ in approximately $[0, 0.3]$ and $[1.3, 1.5]$ (the dotted lines), there exists another value with the same coefficient $\alpha$. Accordingly, the decoder will find the same curve for two different parameters, which is wrong since $p_d$ defines a shift. This explains why the DAE can interpolate well in the top left subplot in Figure \ref{fig:sin_shift_test}, but not in the top right one. These results also show what is meant by ``holes'' in the latent space for DAEs.

On the other hand, as proposed earlier, a longer latent space allows us to find better coefficients. This is clearly shown by the coefficients of the strong method in Figure \ref{fig:sin_shift_latent} (left), which have a monotonic behavior.

Finally, the right part of the figure depicts that when the curves are simple to separate and are characterized by only one parameter, both architectures can find monotonic coefficients that fit both the train and test sets. We conclude that even though long latent spaces have many advantages, they are not always necessary, especially when the solution is simple and separable.
\section{Testing on Numerical Data}\label{sec:Nuemrical}
The solutions interpolated in the previous section were only characterized by one parameter. In this section, we test RRAEs and compare them to IRMAEs, and LoRAEs on two examples with a parametric space of dimension two, as well as on the MNIST dataset.
\subsection{Examples with two parameters}\label{sec:numerical_example}
We generated two challenging synthetic tests for interpolation. First, we propose the sum of two sine curves with different frequencies, as well as two Gaussian bumps in two different locations. We show how in such examples both our formulations result in latent spaces with a lower rank and better results than IRMAEs and LoRAEs for the hyperparameters chosen (again, training details can be found in Appendix \ref{apdx:train}). We define the columns of our input matrix $X_d(t_v,\, p_d) = f_{prob}$ for each problem as follows,
\begin{equation}\nonumber
    \begin{cases}
        f_{freqs}(t_v,\, \textbf{p}_d) = \sin(p^1_d\pi t_v)+\sin(p^2_d\pi t_v), \qquad\qquad\quad                       & p^1_d \in [0.3, 0.5], \quad p^2_d \in [0.8, 1], \\[1.5ex]
        f_{gauss}(t_v,\, \textbf{p}_d) =  1.3e^{-\frac{(t_v-p^1_d)^2}{0.08}}+1.3e^{-\frac{(t_v-p^2_d)^2}{0.08}}, \qquad & p^1_d \in [1, 3], \quad p^2_d \in [4, 6],
    \end{cases}
\end{equation}
We distinguish between the \textbf{bold} notation for vectors and non-bold ones for scalars. In both expressions, our parametric space is of dimension $2$ and so $\textbf{p}_d = [p^1_d, \,p^2_d] \in \mathbb{R}^2$. For each example and each architecture, we present some interpolated predictions in Figure \ref{fig:sin_sin_gauss}, and the error over all the training/testing sets in Table \ref{fig:table_sin_sin_gauss} as well as the average training time for 100 batches.


\begin{table}[h]
    \caption{Relative error (in \%) for all architectures on both the train and test with a latent space of dimension 2800 for the two examples presented, as well as the average time (in s) for 100 batches.}
    \label{fig:table_sin_sin_gauss}
    \centering
    \begin{tabular}{cccccc}
        \toprule
        {}            & \multicolumn{2}{c}{Mult. Frequencies} & \multicolumn{2}{c}{Mult. Gausses}                                             \\
        \cmidrule(lr){2-5}                                                      \\
        \addlinespace[-0.2cm]
        Model        & Train Error                       & Test error    & Train Error   & Test error  &  Average time   \\
        \midrule
        RRAE (strong)          & 6.33                      & \textbf{12.83} & 4.46 & \textbf{8.75} & 1.61\\
        RRAE (weak)         & 10.33                             &     15.09     & 8.50         & 10.69  & \textbf{0.52} \\
        IRMAE (l=2)        & 6.95                             &      17.35    & 4.68         & 13.93 & 3.6\\
        IRMAE (l=4)        &     N/A                         &     N/A     & 8.41         & 14.78 & 7.50\\
        LoRAE        &    \textbf{5.40}                          &    13.83      &     \textbf{3.03}     &  9.39 & 420.4\\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[!t]
    \centering
    \includegraphics[clip, scale=0.33, trim=1.6cm 1.2cm 0cm 2.4cm]{sin_sin_gauss_plot_test.pdf}
    \caption{Interpolated results of RRAEs with both formulations on both examples presented with bilinear interpolation in the latent space.}
    \label{fig:sin_sin_gauss}
\end{figure}

\begin{figure}[!b]
    \centering
    \includegraphics[clip, scale=0.29, trim=1.6cm 1.3cm 0cm 2.6cm]{sing_vals_gauss.pdf}
    \caption{Normalized singular values of the latent space $Y$ when trained over multiple gausses. The first 40 singular values are shown to the left, while many of the small singular values are shown on a log scale to the right.}
    \label{fig:sin_vals_gauss}
\end{figure}

As can be seen in Table \ref{fig:table_sin_sin_gauss}, RRAEs with the Strong formulation are the most efficient in interpolation. Additionally, we note that increasing the parameter $l$ for the IRMAE leads to divergence of the gradient descent (hence the N/A). Note that we only used the parameters specified in both papers for IRMAE and LoRAE. A fine-tuning of the parameters could lead to better results for these architectures. On the other hand, the table shows that RRAEs are faster than both IRMAEs and LoRAEs for the latent space dimension chosen. Our formulations are fast since we only add small matrices to the loss in the weak formulation, and we compute an SVD of an $L\times bs$ matrix, $bs$ being the batch size, in the Strong formulation. On the other hand, IRMAEs and LoRAEs find the gradient/the nuclear norm of an $L\times L$ matrix respectively, with $L$ relatively large. 

Additionally, we draw parts of the normalized singular values of the latent space for the multiple gausses problem in figure \ref{fig:sin_vals_gauss}. The figure illustrates that adding the number of linear layers for the IRMAE (i.e. increasing $l$) indeed reduces the rank of the latent space. However, both of our formulations can be asked to find latent spaces with lower ranks. In addition, it is important to note that even though the LoRAE has a low error overall, the latent space rank is still relatively high compared to the other techniques (as can be seen from the slowly decreasing singular values in violet in Figure \ref{fig:sin_vals_gauss}). Hence, the trained model can not efficiently be used for other tasks, such as feature extraction.  

From the log graph to the right, we can understand why the strong formulation achieves the best results. As previously mentioned, the IRMAE, the LoRAE and our weak formulation don't enforce the singular values to fall to zero. So even though many singular values are small, they still have a noise effect over the decoder, which reduces their efficiency in interpolation.

\subsection{Testing on MNIST}\label{sec:extensions}
In this section, we compare our architecture to the IRMAE and the LoRAE on the MNIST dataset. An example of linear interpolation of five steps between numbers $5$ and $0$ can be seen in figure \ref{fig:interp_mnist}. The singular values of the latent space for all formulations can be found in figure \ref{fig:sin_vals_mnist}.

The interpolation in figure \ref{fig:interp_mnist} depicts that the Strong formulation is capable of performing better interpolation. While all techniques face problems with the second subplot from the left, the drawing for the strong formulation is the closest to a $5$, while the drawing for the weak formulation could be confused for a $6$, and the corresponding picture for the IRMAE doesn't look like a number.

On the other hand, the singular values drawn in figure \ref{fig:sin_vals_mnist} show that the weak formulation surprisingly converges to a very low rank. It is important to note that both the strong and the weak formulations were able to interpolate on the MNIST dataset with a rank that's strictly less than 6. This shows the ability of our formulations to perform feature extraction. Again, the noise caused by the nonzero singular values is the main reason behind the enhanced capabilities of the strong method to interpolate between numbers.
\begin{figure}[!b]
    \centering
    \includegraphics[clip, scale=0.6, trim=0 0cm -1cm 1cm]{mnist_interp.pdf}
    \caption{Three-step interpolation between a rotated number $5$ and a number $0$ on MNIST using RRAEs with both formulations, IRMAEs, and LoAREs.}
    \label{fig:interp_mnist}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[clip, scale=0.29, trim=1.6cm 1.3cm 0cm 2.6cm]{sing_vals_mnist.pdf}
    \caption{Normalized singular values of the latent space $Y$ when trained over MNIST. The first 40 singular values are shown to the left, while many of the small singular values are shown on a log scale to the right.}
    \label{fig:sin_vals_mnist}
\end{figure}

\section{Limitations}\label{sec:limitations}
As has been shown in the paper's results, RRAEs with both formulations can interpolate well while using a latent space with a low rank. However, our proposed model has some limitations:
\begin{enumerate}
    \item Even though both of our formulations allow explicit control over the rank of the latent space, $k_{max}$ is an additional hyperparameter to be tuned. In practice, we found that starting with a small value of $k_{max}$ and increasing it until error convergence is a good strategy. In general, other techniques such as a PCA could be used to approximate the intrinsic dimension of the latent space.
    \item The weak formulation adds regularisation constants to the loss, which can be hard to tune. However, we found that by monitoring the two terms in the loss while training, it is clear in the first few epochs which constant should be increased/decreased. Yet, training should be performed multiple times to tune the weights of each term in the loss.
    \item For high dimensional problems where a very large latent space is needed, the strong formulation can be computationally expensive. Even though the SVD is only performed on a matrix of size $L\times bs$, $bs$ being the batch size and $L$ the length of the latent space, the cost of computing an SVD and backpropagating through it when $L$ is very large can be high.
    \item As shown in Section \ref{sec:insights}, the effect of a long latent space is reduced when the solution is simple and separable. In such cases, an increased dimension of the latent space (and hence the use of RRAEs) might not be necessary. 
\end{enumerate}

\section{Summary and Conclusions}\label{sec:conclusions}
In this article, we presented Rank Reduction Autoencoders (RRAEs), Autoencoders with latent spaces that accept linear reduction. We proposed two formulations, a weak and a strong one to find the latent space while building its reduced basis. Even though the basis vectors in the strong formulation are orthogonal, and they need not be in the weak formulation, we showed that both formulations can interpolate well between curves. 

Overall, our results show that the Strong formulation has a superior capability of interpolation since it doesn't have any noise from small nonzero singular values in the latent space. We also showed that both the Strong and the Weak formulations can achieve much lower ranks in the latent space while being able to efficiently interpolate vector functions. Finally, both formulations are fast to train, with the weak formulation being the fastest. While the Strong formulation leads to better predictions, the Weak formulation is much simpler to implement since it only adds a penalty term to the loss.
\newpage

\bibliographystyle{plainnat}
\bibliography{sn-bibliography}
\newpage
\section*{NeurIPS Paper Checklist}

\begin{enumerate}

\item {\bf Claims}
    \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We include mainly three claims (zero singular values, computational time, and explicit control over the rank), which are all tackled later in the paper. {}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the abstract and introduction do not include the claims made in the paper.
        \item The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. 
        \item The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. 
        \item It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 
    \end{itemize}

\item {\bf Limitations}
    \item[] Question: Does the paper discuss the limitations of the work performed by the authors?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: Section \ref{sec:limitations} basically discusses the main limitations of our proposed model. We also discussed how Diabolo Autoencoders can achieve similar results to RRAEs in certain training conditions in Section \ref{sec:insights}. {}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 
        \item The authors are encouraged to create a separate "Limitations" section in their paper.
        \item The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
        \item The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
        \item The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
        \item The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
        \item If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
        \item While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
    \end{itemize}

\item {\bf Theory Assumptions and Proofs}
    \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The paper does not include theoretical results.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include theoretical results. 
        \item All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
        \item All assumptions should be clearly stated or referenced in the statement of any theorems.
        \item The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. 
        \item Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
        \item Theorems and Lemmas that the proof relies upon should be properly referenced. 
    \end{itemize}

    \item {\bf Experimental Result Reproducibility}
    \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We include all the necessary details for creating the synthetic datasets throughout the article. We also detail all the hyperparameters used for training in Appendix \ref{apdx:train}.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
        \item If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. 
        \item Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
        \item While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
        \begin{enumerate}
            \item If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
            \item If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
            \item If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
            \item We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
        \end{enumerate}
    \end{itemize}


\item {\bf Open access to data and code}
    \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
    \item[] Answer: \answerNo{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: While we can not share the code used, we provide all the necessary details for generating the data and performing training throughout the article and in the appendices.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that paper does not include experiments requiring code.
        \item Please see the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
        \item While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
        \item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
        \item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
        \item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
        \item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
        \item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
    \end{itemize}


\item {\bf Experimental Setting/Details}
    \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We provide the important numbers throughout the article. All the parameters can be seen in Appendix \ref{apdx:train}.

    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
        \item The full details can be provided either with the code, in appendix, or as supplemental material.
    \end{itemize}

\item {\bf Experiment Statistical Significance}
    \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
    \item[] Answer: \answerNo{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: This was not done due to restrictions in time and resources. However, we will be able to provide error bars if seen as necessary by the reviewers. We tested heavily RRAEs on multiple examples, which is why we believe that the results are reliable.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
        \item The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
        \item The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
        \item The assumptions made should be given (e.g., Normally distributed errors).
        \item It should be clear whether the error bar is the standard deviation or the standard error of the mean.
        \item It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96\% CI, if the hypothesis of Normality of errors is not verified.
        \item For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
        \item If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
    \end{itemize}

\item {\bf Experiments Compute Resources}
    \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: All of the details were mentioned in Appendix \ref{apdx:train}.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
        \item The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. 
        \item The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 
    \end{itemize}
    
\item {\bf Code Of Ethics}
    \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics \url{https://neurips.cc/public/EthicsGuidelines}?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The code of ethics has been thoroughly read by the corresponding author who made sure that the research conducted in the paper conforms to it.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
        \item If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
        \item The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
    \end{itemize}


\item {\bf Broader Impacts}
    \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
    \item[] Answer: \answerNo{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: Since we are working on foundational research with no particular applications. We believe our topic is too broad to have any specific societal impacts.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that there is no societal impact of the work performed.
        \item If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
        \item Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
        \item The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
        \item The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
        \item If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
    \end{itemize}
    
\item {\bf Safeguards}
    \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The paper poses no such risks.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper poses no such risks.
        \item Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. 
        \item Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
        \item We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
    \end{itemize}

\item {\bf Licenses for existing assets}
    \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We are the owners of the model presented and we credited the libraries and languages that were used for the code.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not use existing assets.
        \item The authors should cite the original paper that produced the code package or dataset.
        \item The authors should state which version of the asset is used and, if possible, include a URL.
        \item The name of the license (e.g., CC-BY 4.0) should be included for each asset.
        \item For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
        \item If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, \url{paperswithcode.com/datasets} has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
        \item For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
        \item If this information is not available online, the authors are encouraged to reach out to the asset's creators.
    \end{itemize}

\item {\bf New Assets}
    \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The model we propose with both formulations is detailed as a concept in Section \ref{sec:HT}, and the parameters can all be found in Appendix \ref{apdx:train}.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not release new assets.
        \item Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. 
        \item The paper should discuss whether and how consent was obtained from people whose asset is used.
        \item At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
    \end{itemize}

\item {\bf Crowdsourcing and Research with Human Subjects}
    \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? 
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The paper does not involve crowdsourcing or research with human subjects.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
        \item Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. 
        \item According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 
    \end{itemize}

\item {\bf Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects}
    \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The paper does not involve crowdsourcing or research with human subjects.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
        \item Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. 
        \item We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. 
        \item For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
    \end{itemize}

\end{enumerate}

\newpage
\begin{appendices}
\section{Algorithm for the stair-like function}\label{apdx:stair}
\begin{algorithm}[H]
    \SetKwInput{Input}{Input}
    \SetKwInput{Output}{Output}

    \Input{$p_d\in \mathbb{R}$, $t_v \in \mathbb{R}^T, (\text{Ph}_0, \text{Amp}_0,\kappa, y_0, w) \in \mathbb{R}$}
    ~\\
    $\text{Amp}_{p_d} = p_d$\\[1.6ex]
    $\displaystyle\text{Ph}_{p_d} = \text{Ph}_0+\kappa(\text{Amp}_{p_d}-\text{Amp}_0)$\\[1.6ex]
    $g_{p_d}(t_v) = \text{Amp}_{p_d}\sqrt{t_v}\sin(w(t_v-\text{Ph}_{p_d}))-y_0$\\[1.6ex]
    $\displaystyle h_{p_d}(t) = \left(\frac{\left|g_{p_d}(t)\right|+g_{p_d}(t)}{2}\right)^5$\\[1.6ex]
    $X_d(t_v, \, p_d)=\text{cumsum}(h_{p_d}(t_v))$
    ~\\
    ~\\
    \Output{$X_d(t_v, \, p_d)$ for each parameter $p_d$.}
    \caption{Algorithm to find $f_{stair}$ for a list of parameters $\textbf{p}$.}
\end{algorithm}
In this paper, we choose the initial parameters of the stair function to be,
\begin{equation}\nonumber
    \begin{cases}
        \text{Ph}_0 = 0.875, \qquad \text{Amp}_0=1 \\
        \kappa = 2.286, \qquad y_0 = 2.3, \qquad w = 2\pi.
    \end{cases}
\end{equation}
\section{Training details}\label{apdx:train}
\subsection{Hyperparameters for synthetic data and PC properties}
The main purpose of this section is to allow readers to reproduce the results, and share the parameters we used to train the models. In general, the main parameters for RRAEs are the dimension of the latent space, $k_{max}$, the encoder/decoder architectures, learning rates, epochs, and batch sizes. For each problem, we fix the common parameters between all architectures. We try to only change necessary parameters between different examples, to show that training RRAEs, especially with the strong formulation, doesn't require too much hyperparameters tuning. For all problems and all formulations, we use an encoder of depth $1$ and width $64$, and a decoder of depth $6$ and width $64$. Additionally, we use batches of size $20$, the \texttt{softplus} as activation function between all layers, and the \texttt{adabeleif} optimizer, for all problems and formulations. Furthermore, we use multiple learning rates, starting with $1\mathrm{e}{-3}$ and dividing by $10$ until reaching $1\mathrm{e}{-5}$ (3 steps). In each step, we train for 2000 batches. however, we impose stagnation criteria which usually stop training earlier. Throughout the paper, the only two parameters that we vary for RRAEs are the length of the latent space $L$ and a coefficient $\kappa_w$ that changes the learning rates for the trainable matrices of the weak method. In practice, we found that changing the learning rate of the trainable matrices $U$ and $A$ for the weak formulation is easier than changing the weights in the loss. Accordingly, while we use the same learning rate strategy proposed before for the encoder/decoder, we propose to multiply the learning rate by a constant $\kappa_w$ before applying it to the trainable matrices $U$ and $A$. By doing so, we find that there is no need to tune the loss parameters (i.e. both are equal to one). It is important to note that $U$ is normalized at every training step so $A$ captures the coefficients. In Table \ref{fig:fixed_params}, we illustrate the latent space dimension $L$ and the constant $\kappa_w$ used for all the illustrated examples in this work (N/A means the weak method was not used for this example).


\begin{table}[h]
    \caption{Different values of the latent space length $L$ and the constant $\kappa_w$ that are used for all the examples in this work (except MNIST).}
    \label{fig:fixed_params}
    \centering
    \begin{tabular}{ccccc}
        \toprule
        Param.     & Shifts & Stair-like & Mult. Freqs. & Mult. Gauss.  \\
        \midrule
        $L$        & 4500       & 4500  & 2800 & 2800       \\
        $\kappa_w$ & N/A     & N/A   & 0.66   & 0.13      \\
        \bottomrule
    \end{tabular}
\end{table}


Next, we detail how the choice of $k_{max}$ was made for each example. In general, an approximation of $k_{max}$ can be found using multiple techniques (e.g. PCA). However, in this paper, we chose a simpler approach. If this hyperparameter is too small, the model will not converge, but if it is too large, the Neural Network will simply learn a latent space of a higher rank. Accordingly, we started with a small value of $k_{max}$ and increased it until the error converged. The values chosen for each example are detailed in Table \ref{fig:kmax_params}.

\begin{table}[h]
    \caption{Different values of $k_{max}$ that are used for all the examples in this work (except MNIST).}
    \label{fig:kmax_params}
    \centering
    \begin{tabular}{ccccc}
        \toprule
        Param.     & Shifts & Stair-like & Mult. Freqs. & Mult. Gauss.  \\
        \midrule
        $k_{max}$        & 1       & 1  & 12 & 2\\
        \bottomrule
    \end{tabular}
\end{table}

As can be seen in the table, while we were able to choose exactly the dimension of the parametric space for most of the examples, for the sine curves with different frequencies, the method needed a higher rank in the latent space to converge to the low errors presented. This is mainly because the latent space was not long enough for the problem. However, we tried to fix the parameter $L$, so we had to change $k_{max}$ accordingly.

On the other hand, we chose the parameters that were shown to give the best results for the IRMAE and the LoRAE. We tried to have two and four linear layers for the IRMAE (i.e. $l=2$ and $l=4$), and we used a weight of $0.001$ in the loss for the LoRAE. To ensure a fair comparison, every other parameter of these models was chosen to be the same as RRAEs (the ones listed before).

Since we provide average computational times in Section \ref{sec:numerical_example}, we give some details about the machine used to generate these results. The PC used is an MSI Stealth 17Studio A13VH. The processor os intel (13th generation), Core i9, 2600 MHz, 14 CPUs. The PC also has 64 GB of RAM. The library used was \texttt{equinox}, in \texttt{JAX} for the training. However, the code was only run on CPUs.
 
\subsection{Hyperparameters for MNIST}
The architecture for the MNIST dataset was different since convolutional Neural Networks were used. We fixed the kernel size to 4, the padding to 1, and the strid to 2 for each convolusion/convolusion transpose. For the encoder, we used convolutions with output 32, 64, 128, and 256 respectively, with relu activation functions in between. These were followed by a flattening layer, and a Multilayer Perceptron (MLP) with softplus activation functions, of depth 2, and width 64. The output of the MLP was fixed to be 128, the dimension of the latent space.

On the other hand, the decoder included an MLP with depth 2, width 64, and softplus activation functions with an output dimension of 1568. The vector was then reshaped into a tensor of shape $(32, 7, 7)$, which was then followed by two transposed convolutions with output shapes 8, and 1 respectively.

Even though the architecture is probably not the best one, our purpose was to show that for a fixed architecture, RRAEs can outperform other existing methods and not achieve SOTA results over the MNIST.
\newpage
\subsection{Choice of parameters}
Throughout the paper, we mentioned the range in which the values of $\textbf{p}_d$ were chosen for each example. In this subsection, we provide some details on the chosen values of $\textbf{p}_d$, mainly to show that the test covers most of the parametric space. Throughout the paper, we presented curves with parametric spaces of one and two. The following figures show the plot of the second parameter against the first one when the space is of dimension two (Figure \ref{fig:two_p}). On the other hand, when the curve is only characterized by one parameter, we plot the vector of the parameter against itself (Figure \ref{fig:one_p}). Hence, we plot dots on a diagonal line to show where the test values lie compared to the train values. Our test set was chosen randomly but using a \texttt{JAX} seed to ensure reproducibility. As can be seen in the figures, we carefully chose the seeds and the number of tests to represent most of the parametric space.


\begin{figure}[!h]
    \centering
    \includegraphics[clip, trim=1cm 0cm 0cm 3cm, scale=0.28]{p_vals_1.pdf}
    \caption{Train and test parameter values for the example with two shifted sine curves (left), and stair-like curves (right).}
    \label{fig:one_p}

\end{figure}


\begin{figure}[!h]
    \centering
    \includegraphics[clip, trim=1cm 0cm 0cm 3cm, scale=0.28]{p_vals_2.pdf}
    \caption{Train and test parameter values for the example with two accelerated sine curves (left), and two Gausses (right).}
    \label{fig:two_p}
\end{figure}

\section{Comparing RRAEs to AEs with long latent dimensions}
Throughout the paper, we only compared our proposed models with others that allow feature extraction, since it is of interest to us. However, Vanilla Autoencoders with long latent spaces can interpolate very well! On both the multiple frequencies and the multiple gausses, the comparison between our Strong formulation and a Vanilla Autoencoder with a latent space of dimension $2800$ is shown in table \ref{fig:table_vanilla}.

\begin{table}[h]
    \caption{Error (in \%) on our two synthetic problems for our Strong formulation and an AE with the same latent dimension with no rank restriction.}
    \label{fig:table_vanilla}
    \centering
    \begin{tabular}{ccccc}
        \toprule
        {}            & \multicolumn{2}{c}{Mult. Freqs} & \multicolumn{2}{c}{Mult Gausses}                                \\
        \cmidrule(lr){2-5}                                                                                               \\
        \addlinespace[-0.2cm]
        Model         & Train Error                      & Test error                     & Train Error   & Test error   \\
        \midrule
        AE (long)    &    7.96                         & 14.08                          & \textbf{3.37}         & 10.56         \\
        RRAE (strong) & \textbf{6.33}                    & \textbf{12.83}                   & 4.46 & \textbf{8.75} \\
        \bottomrule
    \end{tabular}
\end{table}
As can be seen in the table, reducing the rank not only allows feature extraction, it can also help in training to achieve better results and interpolation.
\section{Batching}
In this section, we detail how batching was performed for both the Weak and the Strong formulations.

\underline{The weak formulation:} We remind the reader that the weak formulation had the norm of $Y - UA$in the loss, with $Y \in \mathbb{R}^{L \times D}$, $U \in \mathbb{R}^{L \times k_{max}}$, and $A \in \mathbb{R}^{k_{max}\times D}$. However, when training over batches of size $bs$, we have a batched latent space $Y^b\in \mathbb{R}^{L \times bs}$ and so the shape of $A$ needs to be different. Accordingly, for each batch, we keep the indicies of the vector functions used, and take the column of the same indices from $A$ to form $A_b \in \mathbb{R}^{k_{max}\times bs}$. Accordingly, for each forward/backward pass, we train different columns of matrix $A$.

\underline{The strong formulation:} For the strong formulation, nothing has changed. The truncated SVD is performed over the batched latent space $Y_b=U\Sigma V$. It is important to note though that since the columns change depending on the batch, the values of the right singular vector (i.e. $V$) fluctuate a lot in training. However, since the same vector $U$ is used for all the batches, the RRAE converges towards the right basis vectors in $U$. However, to be sure that training is performed over the whole dataset after training is completed, we perform the truncated SVD over all the curves to get the corresponding reduced basis and coefficients that are used for interpolation. Our findings are that the RRAE converges towards a unique U, which is why interpolation is so successful throughout the paper.

\end{appendices}
\end{document}