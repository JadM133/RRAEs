\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\catcode 95\active
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{tezzele2019shape}
\citation{nguyen2022efficient}
\citation{rama2020towards}
\citation{chinesta2011short}
\citation{sancarlos2023regularized}
\citation{rowley2004model}
\citation{kerschen2005method}
\citation{ladeveze1985famille}
\citation{rodriguez2019non}
\citation{chinesta2014pgd}
\citation{labrin2020principal}
\citation{gonzalez2018kpca}
\citation{TORREGROSA202212}
\citation{TORREGROSA202236}
\citation{srebro2003weighted}
\citation{liu2012robust}
\citation{udell2016generalized}
\citation{davenport2016overview}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\citation{berrada2020training}
\citation{rigol2001artificial}
\citation{hinton1993autoencoders}
\citation{vachhani2017deep}
\citation{feng2014speech}
\citation{deng2019towards}
\citation{park2018multimodal}
\citation{bank2023autoencoders}
\citation{doersch2016tutorial}
\citation{ng2011sparse}
\citation{jing2020implicit}
\citation{mazumder2023learning}
\citation{brunton2021modern}
\citation{kpca}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{2}{Introduction}{section.1}{}}
\citation{stewart1993early}
\citation{nakatsukasa2017accuracy}
\@writefile{toc}{\contentsline {section}{\numberline {2}Rank Reduction Autoencoders (RRAEs)}{3}{section.2}\protected@file@percent }
\newlabel{sec:HT}{{2}{3}{Rank Reduction Autoencoders (RRAEs)}{section.2}{}}
\newlabel{eqn:trunc}{{3}{3}{Rank Reduction Autoencoders (RRAEs)}{equation.2.3}{}}
\newlabel{eqn:alphas}{{4}{3}{Rank Reduction Autoencoders (RRAEs)}{equation.2.4}{}}
\newlabel{eqn:mat_form}{{5}{3}{Rank Reduction Autoencoders (RRAEs)}{equation.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Schematic showing the autoencoder in use as well as both methodologies. There are two terms in the loss function for the \textcolor {myblue}{Weak formulation}. On the other hand, there's an additional step before the decoder for the \textcolor {myred}{Strong formulation}.}}{4}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:mdoel_arch}{{1}{4}{Schematic showing the autoencoder in use as well as both methodologies. There are two terms in the loss function for the \textcolor {myblue}{Weak formulation}. On the other hand, there's an additional step before the decoder for the \textcolor {myred}{Strong formulation}}{figure.caption.1}{}}
\citation{jing2020implicit}
\@writefile{toc}{\contentsline {section}{\numberline {3}Interpolation in the latent space}{5}{section.3}\protected@file@percent }
\newlabel{sec:interp}{{3}{5}{Interpolation in the latent space}{section.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Figure showing the result when interpolating linearly between two shifted sine curves, showing why linear interpolation is not a good choice for problems with multiple dominant singular values.}}{5}{figure.caption.2}\protected@file@percent }
\newlabel{fig:sin}{{2}{5}{Figure showing the result when interpolating linearly between two shifted sine curves, showing why linear interpolation is not a good choice for problems with multiple dominant singular values}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Insights behind RRAEs}{6}{section.4}\protected@file@percent }
\newlabel{sec:insights}{{4}{6}{Insights behind RRAEs}{section.4}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Algorithm to find $f_{stair}$ for a list of parameters $\textbf  {p}$.}}{6}{algocf.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Table showing the relative error (in \%) for all three architectures on both the train and test set for both the examples of shifted sin curves and stair-like ones.}}{7}{table.caption.3}\protected@file@percent }
\newlabel{fig:table_shift_sin}{{1}{7}{Table showing the relative error (in \%) for all three architectures on both the train and test set for both the examples of shifted sin curves and stair-like ones}{table.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Figure showing the predictions of Vanilla Autoencoders and RRAEs with both formulations over two particular values of $p_d$ for the shifted sine (above) and the stair-like examples (below).}}{7}{figure.caption.4}\protected@file@percent }
\newlabel{fig:sin_shift_test}{{3}{7}{Figure showing the predictions of Vanilla Autoencoders and RRAEs with both formulations over two particular values of $p_d$ for the shifted sine (above) and the stair-like examples (below)}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Figure showing the coefficients to be interpolated (dots) for all three architectures, and the interpolated values for the test set (crosses) for the shifted sine curves (left) and the stair-like curves after normalization (right).}}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig:sin_shift_latent}{{4}{8}{Figure showing the coefficients to be interpolated (dots) for all three architectures, and the interpolated values for the test set (crosses) for the shifted sine curves (left) and the stair-like curves after normalization (right)}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Testing on Numerical Data}{9}{section.5}\protected@file@percent }
\newlabel{sec:Nuemrical}{{5}{9}{Testing on Numerical Data}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Examples with one/two parameters}{9}{subsection.5.1}\protected@file@percent }
\newlabel{sec:numerical_example}{{5.1}{9}{Examples with one/two parameters}{subsection.5.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Table showing the relative error (in \%) for all three architectures on both the train and test set for the three examples with one/two parameters.}}{9}{table.caption.7}\protected@file@percent }
\newlabel{fig:table_sin_sin_gauss}{{2}{9}{Table showing the relative error (in \%) for all three architectures on both the train and test set for the three examples with one/two parameters}{table.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Extensions of RRAES}{9}{subsection.5.2}\protected@file@percent }
\newlabel{sec:extensions}{{5.2}{9}{Extensions of RRAES}{subsection.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Figure showing the interpolated results of RRAEs with both formulations, as well as a Vanilla AE on the three examples presented with linear/bilinear interpolation in the latent space.}}{10}{figure.caption.6}\protected@file@percent }
\newlabel{fig:sin_sin_gauss}{{5}{10}{Figure showing the interpolated results of RRAEs with both formulations, as well as a Vanilla AE on the three examples presented with linear/bilinear interpolation in the latent space}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Figure showing the interpolated results of RRAEs with both formulations on the crystallization rates with a maximum rank of $10$ (left) and with noise (right).}}{11}{figure.caption.8}\protected@file@percent }
\newlabel{fig:crystallization}{{6}{11}{Figure showing the interpolated results of RRAEs with both formulations on the crystallization rates with a maximum rank of $10$ (left) and with noise (right)}{figure.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Table showing the relative error (in \%) when different ranks are enforced for the latent space to learn interpolation over the crystallization rates.}}{11}{table.caption.9}\protected@file@percent }
\newlabel{fig:table_avramis}{{3}{11}{Table showing the relative error (in \%) when different ranks are enforced for the latent space to learn interpolation over the crystallization rates}{table.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Figure showing the first five singular values of the latent spaced normalized when different ranks of the latent space are enforced with the strong (left) and the weak (right) formulations.}}{12}{figure.caption.10}\protected@file@percent }
\newlabel{fig:sing_vals_avrami}{{7}{12}{Figure showing the first five singular values of the latent spaced normalized when different ranks of the latent space are enforced with the strong (left) and the weak (right) formulations}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Summary and Conclusions}{12}{section.6}\protected@file@percent }
\newlabel{sec:conclusions}{{6}{12}{Summary and Conclusions}{section.6}{}}
\bibstyle{unsrt}
\bibdata{sn-bibliography}
\bibcite{tezzele2019shape}{1}
\bibcite{nguyen2022efficient}{2}
\bibcite{rama2020towards}{3}
\bibcite{chinesta2011short}{4}
\bibcite{sancarlos2023regularized}{5}
\bibcite{rowley2004model}{6}
\bibcite{kerschen2005method}{7}
\bibcite{ladeveze1985famille}{8}
\bibcite{rodriguez2019non}{9}
\bibcite{chinesta2014pgd}{10}
\bibcite{labrin2020principal}{11}
\bibcite{gonzalez2018kpca}{12}
\bibcite{TORREGROSA202212}{13}
\bibcite{TORREGROSA202236}{14}
\bibcite{srebro2003weighted}{15}
\bibcite{liu2012robust}{16}
\bibcite{udell2016generalized}{17}
\bibcite{davenport2016overview}{18}
\bibcite{berrada2020training}{19}
\bibcite{rigol2001artificial}{20}
\bibcite{hinton1993autoencoders}{21}
\bibcite{vachhani2017deep}{22}
\bibcite{feng2014speech}{23}
\bibcite{deng2019towards}{24}
\bibcite{park2018multimodal}{25}
\bibcite{bank2023autoencoders}{26}
\bibcite{doersch2016tutorial}{27}
\bibcite{ng2011sparse}{28}
\bibcite{jing2020implicit}{29}
\bibcite{mazumder2023learning}{30}
\bibcite{brunton2021modern}{31}
\bibcite{kpca}{32}
\bibcite{stewart1993early}{33}
\bibcite{nakatsukasa2017accuracy}{34}
\gdef \@abspage@last{17}
