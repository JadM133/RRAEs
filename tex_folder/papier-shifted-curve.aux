\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\catcode 95\active
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\citation{zienkiewicz2005finite}
\citation{smith1985numerical}
\citation{rowley2004model}
\citation{kerschen2005method}
\citation{ladeveze1985famille}
\citation{rodriguez2019non}
\citation{chinesta2014pgd}
\citation{labrin2020principal}
\citation{gonzalez2018kpca}
\citation{tezzele2019shape}
\citation{nguyen2022efficient}
\citation{rama2020towards}
\citation{chinesta2011short}
\citation{sancarlos2023regularized}
\citation{srebro2003weighted}
\citation{liu2012robust}
\citation{udell2016generalized}
\citation{davenport2016overview}
\citation{popescu2009multilayer}
\citation{seiffert2001multiple}
\citation{mitra1995fuzzy}
\citation{tang2015extreme}
\citation{medsker2001recurrent}
\citation{salehinejad2017recent}
\citation{medsker1999recurrent}
\citation{albawi2017understanding}
\citation{gu2018recent}
\citation{li2021survey}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{2}{Introduction}{section.1}{}}
\citation{berrada2020training}
\citation{rigol2001artificial}
\citation{hinton1993autoencoders}
\citation{vachhani2017deep}
\citation{feng2014speech}
\citation{deng2019towards}
\citation{shin2012stacked}
\citation{park2018multimodal}
\citation{sergeant2015multimodal}
\citation{bank2023autoencoders}
\citation{lusch2018deep}
\citation{wang2016auto}
\citation{jing2020implicit}
\citation{mazumder2023learning}
\citation{stewart1993early}
\citation{nakatsukasa2017accuracy}
\@writefile{toc}{\contentsline {section}{\numberline {2}Rank Reduction Autoencoders (RRAEs)}{4}{section.2}\protected@file@percent }
\newlabel{sec:HT}{{2}{4}{Rank Reduction Autoencoders (RRAEs)}{section.2}{}}
\newlabel{eqn:trunc}{{4}{4}{Rank Reduction Autoencoders (RRAEs)}{equation.2.4}{}}
\newlabel{eqn:alphas}{{5}{4}{Rank Reduction Autoencoders (RRAEs)}{equation.2.5}{}}
\newlabel{eqn:mat_form}{{6}{4}{Rank Reduction Autoencoders (RRAEs)}{equation.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Schematic showing the autoencoder in use as well as both methodologies. There are two terms in the loss function for the \textcolor {myblue}{Weak formulation}. On the other hand, there's an additional step before the decoder for the \textcolor {myred}{Strong formulation}.}}{5}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:mdoel_arch}{{1}{5}{Schematic showing the autoencoder in use as well as both methodologies. There are two terms in the loss function for the \textcolor {myblue}{Weak formulation}. On the other hand, there's an additional step before the decoder for the \textcolor {myred}{Strong formulation}}{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Interpolation in the latent space}{6}{section.3}\protected@file@percent }
\newlabel{sec:interp}{{3}{6}{Interpolation in the latent space}{section.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Figure showing the result when interpolating linearly between two shifted sin curves, showing why linear interpolation is not a good idea for problems with multiple dominant singular values.}}{6}{figure.caption.2}\protected@file@percent }
\newlabel{fig:sin}{{2}{6}{Figure showing the result when interpolating linearly between two shifted sin curves, showing why linear interpolation is not a good idea for problems with multiple dominant singular values}{figure.caption.2}{}}
\citation{jing2020implicit}
\citation{mazumder2023learning}
\@writefile{toc}{\contentsline {section}{\numberline {4}Insights behind RRAEs}{7}{section.4}\protected@file@percent }
\newlabel{sec:insights}{{4}{7}{Insights behind RRAEs}{section.4}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Algorithm to find $f_{stair}$ for a list of parameters $\textbf  {p}$.}}{9}{algocf.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Table showing the relative error (in \%) for all three architectures on both the train and test set for shifted sin curves.}}{9}{table.caption.3}\protected@file@percent }
\newlabel{fig:table_shift_sin}{{1}{9}{Table showing the relative error (in \%) for all three architectures on both the train and test set for shifted sin curves}{table.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Figure showing the predictions of Vanilla Autoencoders and RRAEs with both formulations of four particular values of $p$ in the test set.}}{10}{figure.caption.4}\protected@file@percent }
\newlabel{fig:sin_shift_test}{{3}{10}{Figure showing the predictions of Vanilla Autoencoders and RRAEs with both formulations of four particular values of $p$ in the test set}{figure.caption.4}{}}
\citation{jing2020implicit}
\citation{mazumder2023learning}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Figure showing the coefficients to be interpolated (dots) for all three architectures, and the interpolated values for the test set (crosses).}}{11}{figure.caption.5}\protected@file@percent }
\newlabel{fig:sin_shift_latent}{{4}{11}{Figure showing the coefficients to be interpolated (dots) for all three architectures, and the interpolated values for the test set (crosses)}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Testing on Numerical Data}{12}{section.5}\protected@file@percent }
\newlabel{sec:Nuemrical}{{5}{12}{Testing on Numerical Data}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Examples with one/two parameters}{12}{subsection.5.1}\protected@file@percent }
\newlabel{sec:numerical_example}{{5.1}{12}{Examples with one/two parameters}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Extensions of RRAES}{12}{subsection.5.2}\protected@file@percent }
\newlabel{sec:physical}{{5.2}{12}{Extensions of RRAES}{subsection.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Summary and Conclusions}{12}{section.6}\protected@file@percent }
\newlabel{sec:conclusions}{{6}{12}{Summary and Conclusions}{section.6}{}}
\bibstyle{unsrt}
\bibdata{sn-bibliography}
\bibcite{zienkiewicz2005finite}{1}
\bibcite{smith1985numerical}{2}
\bibcite{rowley2004model}{3}
\bibcite{kerschen2005method}{4}
\bibcite{ladeveze1985famille}{5}
\bibcite{rodriguez2019non}{6}
\bibcite{chinesta2014pgd}{7}
\bibcite{labrin2020principal}{8}
\bibcite{gonzalez2018kpca}{9}
\bibcite{tezzele2019shape}{10}
\bibcite{nguyen2022efficient}{11}
\bibcite{rama2020towards}{12}
\bibcite{chinesta2011short}{13}
\bibcite{sancarlos2023regularized}{14}
\bibcite{srebro2003weighted}{15}
\bibcite{liu2012robust}{16}
\bibcite{udell2016generalized}{17}
\bibcite{davenport2016overview}{18}
\bibcite{popescu2009multilayer}{19}
\bibcite{seiffert2001multiple}{20}
\bibcite{mitra1995fuzzy}{21}
\bibcite{tang2015extreme}{22}
\bibcite{medsker2001recurrent}{23}
\bibcite{salehinejad2017recent}{24}
\bibcite{medsker1999recurrent}{25}
\bibcite{albawi2017understanding}{26}
\bibcite{gu2018recent}{27}
\bibcite{li2021survey}{28}
\bibcite{berrada2020training}{29}
\bibcite{rigol2001artificial}{30}
\bibcite{hinton1993autoencoders}{31}
\bibcite{vachhani2017deep}{32}
\bibcite{feng2014speech}{33}
\bibcite{deng2019towards}{34}
\bibcite{shin2012stacked}{35}
\bibcite{park2018multimodal}{36}
\bibcite{sergeant2015multimodal}{37}
\bibcite{bank2023autoencoders}{38}
\bibcite{lusch2018deep}{39}
\bibcite{wang2016auto}{40}
\bibcite{jing2020implicit}{41}
\bibcite{mazumder2023learning}{42}
\bibcite{stewart1993early}{43}
\bibcite{nakatsukasa2017accuracy}{44}
\gdef \@abspage@last{17}
